{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random, math\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bible-kjv.txt'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = gutenberg.sents(gutenberg.fileids()[3])\n",
    "pattern = re.compile(\"[A-Za-z]+\")\n",
    "stop_w =  set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for sent in samples:\n",
    "    sent = [w.lower() for w in sent]\n",
    "    sent = [w for w in sent if w not in stop_w]\n",
    "    sent = [w.replace('\\n', ' ') for w in sent]\n",
    "    sent = [w for w in sent if pattern.fullmatch(w)]\n",
    "    if len(sent) > 5:\n",
    "        corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25481"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(corpus, min_count=5, threshold=3)\n",
    "bigram_phraser = Phraser(bigram)\n",
    "corpus = bigram_phraser[corpus]\n",
    "\n",
    "trigram = Phrases(corpus, min_count=5, threshold=3)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "corpus = trigram_phraser[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(\n",
    "        min_count=3,\n",
    "        window=5,\n",
    "        size=100,\n",
    "        alpha=0.005,\n",
    "        min_alpha=0.0007,\n",
    "        hs=1,\n",
    "        sg=0,\n",
    "        workers=4,\n",
    "        batch_words=100,\n",
    "        cbow_mean = 1\n",
    "    )\n",
    "w2v_model.build_vocab(corpus) # build huffman tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13122157, 14494800)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(\n",
    "        corpus,\n",
    "        total_examples=w2v_model.corpus_count,\n",
    "        epochs=50,\n",
    "        report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chiangchiuti/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('jesus_christ', 0.8696889877319336),\n",
       " ('christ_jesus', 0.844677746295929),\n",
       " ('faith', 0.8157445192337036),\n",
       " ('gospel', 0.8098512291908264),\n",
       " ('believe', 0.788714587688446),\n",
       " ('justified', 0.7848519682884216),\n",
       " ('faithful', 0.7752644419670105),\n",
       " ('lord_jesus_christ', 0.7724319696426392),\n",
       " ('world', 0.7681786417961121),\n",
       " ('circumcision', 0.7642426490783691)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"christ\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weighted sum of word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_log_probs(model, target_w, context_embd: np.ndarray)-> np.ndarray:\n",
    "    turns = (-1.0) ** target_w.code\n",
    "    path_embd = model.trainables.syn1[target_w.point]\n",
    "    log_probs = -np.logaddexp(0, -turns * np.dot(context_embd, path_embd.T))\n",
    "    return np.sum(log_probs)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "def _cal_keyword_score(model, sentence:List[str]) -> Dict[str, float]:\n",
    "    word_vocabs = [model.wv.vocab[w] for w in sentence if w in model.wv.vocab]\n",
    "    \n",
    "    word_importance = {}\n",
    "    for pos_center, center_w in enumerate(word_vocabs):\n",
    "        context_w_indices = [w.index for pos_w, w in enumerate(word_vocabs) if pos_center != pos_w]\n",
    "        context_embed = np.mean(model.wv.vectors[context_w_indices], axis=0)\n",
    "        log_probs = cal_log_probs(model, center_w, context_embed)\n",
    "        \n",
    "        center_w_term = w2v_model.wv.index2word[center_w.index]\n",
    "        word_importance[center_w_term] = word_importance.get(center_w_term, 0) + log_probs\n",
    "    return word_importance\n",
    "\n",
    "def cal_keyword_score(model, sentence: List[str]) -> np.ndarray:\n",
    "    word_importance = _cal_keyword_score(model, sentence)\n",
    "    ds = pd.Series(word_importance).sort_values(ascending=False)\n",
    "    \n",
    "    scalar = MinMaxScaler(feature_range=(0.1, 1))\n",
    "    array = ds.to_numpy()\n",
    "    array = scalar.fit_transform(array.reshape(array.shape[0], -1))\n",
    "    ds = pd.Series(array.reshape(-1, ), index=ds.index)\n",
    "    return ds\n",
    "\n",
    "def weighted_sum_w2v(model, ds: pd.Series) -> np.ndarray:\n",
    "    ds_  = ds.copy() / sum(ds)\n",
    "    w2v = w2v_model.wv[ds_.index]\n",
    "    weights = np.expand_dims(ds_.to_numpy(), 1)\n",
    "    \n",
    "    return np.sum((w2v * weights), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unto', 'sons', 'concubines', 'abraham', 'abraham', 'gave', 'gifts', 'sent_away', 'isaac', 'son', 'yet', 'lived', 'eastward', 'unto', 'east', 'country']\n",
      "sons          1.000000\n",
      "son           0.954965\n",
      "isaac         0.903312\n",
      "gave          0.781142\n",
      "yet           0.689830\n",
      "lived         0.501238\n",
      "sent_away     0.460567\n",
      "concubines    0.450501\n",
      "gifts         0.429287\n",
      "abraham       0.339437\n",
      "country       0.326302\n",
      "eastward      0.201623\n",
      "unto          0.100229\n",
      "east          0.100000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = corpus[535]\n",
    "\n",
    "ds = cal_keyword_score(w2v_model, sent)\n",
    "print(sent), print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_vector = weighted_sum_w2v(w2v_model, ds)\n",
    "weighted_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.57048685])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2norm = np.linalg.norm(weighted_vector, 2, axis=0, keepdims=True)\n",
    "l2norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_weighted_vector = weighted_vector / l2norm\n",
    "np.linalg.norm(norm_weighted_vector, 2, axis=0, keepdims=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

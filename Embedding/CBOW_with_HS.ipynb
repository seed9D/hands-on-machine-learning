{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import OrderedDict \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import brown, gutenberg\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples  =gutenberg.sents(gutenberg.fileids()[0])\n",
    "pattern = re.compile(\"[A-Za-z]+\")\n",
    "stop_w =  set(stopwords.words('english'))\n",
    "corpus = []\n",
    "for sent in samples:\n",
    "    sent = [w.lower() for w in sent]\n",
    "    sent = [w for w in sent if w not in stop_w]\n",
    "    sent = [w.replace('\\n', ' ') for w in sent]\n",
    "    sent = [w for w in sent if pattern.fullmatch(w)]\n",
    "    if len(sent) > 5:\n",
    "        corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_dist = FreqDist()\n",
    "for sent in corpus:\n",
    "    fre_dist.update(sent)\n",
    "fre_dist = {k : v for k, v in fre_dist.items() if v > 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(fre_dist)\n",
    "idx_to_word = {idx: word for idx,  word in enumerate(fre_dist.keys())}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert word to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indexed = [[word_to_idx[word] for word in sent if word in word_to_idx]for sent in corpus]\n",
    "corpus_indexed = [sent for sent in corpus_indexed if len(sent) > 5]\n",
    "fre_dist_indexed = {word_to_idx[w]: f for w, f in fre_dist.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanNode:\n",
    "    def __init__(self, is_leaf, value=None, fre=0, left=None, right=None):\n",
    "        self.is_leaf = is_leaf\n",
    "        self.value = value\n",
    "        self.fre = fre\n",
    "        self.code = []\n",
    "        self.code_len = 0\n",
    "        self.node_path = []\n",
    "        self.left = left\n",
    "        self.right = right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way builing huffman tree refer to c's original implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanTree:\n",
    "    def __init__(self, fre_dict):\n",
    "        self.root = None\n",
    "        freq_dict = sorted(fre_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "        self.vocab_size = len(freq_dict)\n",
    "        self.node_dict = {}\n",
    "        self._build_tree(freq_dict)\n",
    "    \n",
    "    def _build_tree(self, freq_dict):\n",
    "        # freq_dict is in decent order\n",
    "        # node_list: two part: [leaf node :: internal node]\n",
    "        # leaf node is in decent order; \n",
    "        node_list = [HuffmanNode(is_leaf=True, value=w, fre=fre) for w, fre in freq_dict]\n",
    "        node_list += [HuffmanNode(is_leaf=False, fre=1e10) for i in range(self.vocab_size)]\n",
    "\n",
    "        parentNode = [0] * (self.vocab_size * 2)  # only 2 * vocab_size - 2 be use\n",
    "        binary = [0] * (self.vocab_size * 2)\n",
    "        \n",
    "        pos1 = self.vocab_size - 1\n",
    "        pos2 = self.vocab_size\n",
    "        \n",
    "        # min2i is always larger than min1i\n",
    "        min1i = 0\n",
    "        min2i = 0\n",
    "        for a in range(self.vocab_size - 1):\n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min1i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min1i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min1i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            if pos1 >= 0:\n",
    "                if node_list[pos1].fre < node_list[pos2].fre:\n",
    "                    min2i = pos1\n",
    "                    pos1 -= 1\n",
    "                else:\n",
    "                    min2i = pos2\n",
    "                    pos2 += 1\n",
    "            else:\n",
    "                min2i = pos2\n",
    "                pos2 += 1\n",
    "            \n",
    "            node_list[self.vocab_size + a].fre = node_list[min1i].fre + node_list[min2i].fre\n",
    "            node_list[self.vocab_size + a].left = node_list[min1i]\n",
    "            node_list[self.vocab_size + a].right = node_list[min2i]\n",
    "            \n",
    "            parentNode[min1i] = self.vocab_size + a  # max index = 2 * vocab_size - 2\n",
    "            parentNode[min2i] = self.vocab_size + a\n",
    "            binary[min2i] = 1\n",
    "        \n",
    "        # generate huffman code\n",
    "        for a in range(self.vocab_size):\n",
    "            b = a\n",
    "            i = 0\n",
    "            code = []\n",
    "            point = []\n",
    "            # backtrace node from leaf to root\n",
    "            while b != self.vocab_size * 2 - 2:   # trace path from current node until root node . 'root node index' = 2 * vocab_size - 2 \n",
    "                code.append(binary[b])  \n",
    "                b = parentNode[b]\n",
    "                # point recording the path index from leaf node to root, the length of point is less one than the length of code\n",
    "                point.append(b)\n",
    "            \n",
    "            \n",
    "            node_list[a].code_len = len(code)\n",
    "            node_list[a].code = list(reversed(code))\n",
    "            \n",
    "            # recording the index from root to leaf node, the actually index value should be shifted by self.vocab_size\n",
    "            # In case of full binary tree, the number of non leaf node always is vocab_size - 1, \n",
    "            # The root node of BST in node_list is 2 * vocab_size - 2, and we shift vocab_size to get the actual index of root node: vocab_size - 2\n",
    "            node_list[a].node_path = list(reversed([p - self.vocab_size for p in point]))\n",
    "            \n",
    "            self.node_dict[node_list[a].value] = node_list[a]\n",
    "            \n",
    "        self.root = node_list[2 * vocab_size - 2]\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW + HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, windows_size=2, sentence_length_threshold=5):\n",
    "        self.windows_size = windows_size\n",
    "        self.sentence_length_threshold = sentence_length_threshold\n",
    "        self.contexts, self.centers = self._generate_pairs(corpus, windows_size)\n",
    "        \n",
    "    def _generate_pairs(self, corpus, windows_size):\n",
    "        contexts = []\n",
    "        centers = []\n",
    "        \n",
    "        for sent in corpus:\n",
    "            if len(sent) < self.sentence_length_threshold:\n",
    "                continue\n",
    "            \n",
    "            for center_word_pos in range(len(sent)):\n",
    "                context = []\n",
    "                for w in range(-windows_size, windows_size + 1):\n",
    "                    context_word_pos = center_word_pos + w\n",
    "                    if(0 <= context_word_pos < len(sent) and context_word_pos != center_word_pos):\n",
    "                        context.append(sent[context_word_pos])\n",
    "                if(len(context) == 2 * self.windows_size):\n",
    "                    contexts.append(context)\n",
    "                    centers.append(sent[center_word_pos])\n",
    "        return contexts, centers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return np.array(self.contexts[index]), np.array([self.centers[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSoftmaxLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, freq_dict):\n",
    "        super().__init__()\n",
    "        ## in w2v c implement, syn1 initial with all zero\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.syn1 = nn.Embedding(\n",
    "            num_embeddings=vocab_size + 1,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=vocab_size\n",
    "            \n",
    "        )\n",
    "        torch.nn.init.constant_(self.syn1.weight.data, val=0)\n",
    "        self.huffman_tree = HuffmanTree(freq_dict)\n",
    "\n",
    "    def forward(self, neu1, target):\n",
    "        # neu1: [b_size, embedding_dim]\n",
    "        # target: [b_size, 1]\n",
    "        \n",
    "        # turns:[b_size, max_code_len_in_batch]\n",
    "        # paths: [b_size, max_code_len_in_batch]\n",
    "        turns, paths = self._get_turns_and_paths(target)\n",
    "        paths_emb = self.syn1(paths) # [b_size, max_code_len_in_batch, embedding_dim]\n",
    "\n",
    "   \n",
    "        loss = -F.logsigmoid(\n",
    "            (turns.unsqueeze(2) * paths_emb * neu1.unsqueeze(1)).sum(2)).sum(1).mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def _get_turns_and_paths(self, target):\n",
    "        turns = []  # turn right(1) or turn left(-1) in huffman tree\n",
    "        paths = []\n",
    "        max_len = 0\n",
    "        for n in target:\n",
    "            n = n.item()\n",
    "            node = self.huffman_tree.node_dict[n]\n",
    "            \n",
    "            code = target.new_tensor(node.code).int()  # in code, left node is 0; right node is 1\n",
    "            turn = torch.where(code == 1, code, -torch.ones_like(code))\n",
    "            \n",
    "            turns.append(turn)\n",
    "            paths.append(target.new_tensor(node.node_path))\n",
    "            \n",
    "            if node.code_len > max_len:\n",
    "                max_len = node.code_len\n",
    "        \n",
    "        \n",
    "        turns = [F.pad(t, pad=(0, max_len - len(t)), mode='constant', value=0) for t in turns] \n",
    "        paths = [F.pad(p, pad=(0, max_len - p.shape[0]), mode='constant', value=net.hs.vocab_size) for p in paths]\n",
    "        return torch.stack(turns).int(), torch.stack(paths).long()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWHierarchicalSoftmax(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, fre_dict):\n",
    "        super().__init__()\n",
    "        self.syn0 = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hs = HierarchicalSoftmaxLayer(vocab_size, embedding_dim, fre_dict)\n",
    "        torch.nn.init.xavier_uniform_(self.syn0.weight.data)\n",
    "    \n",
    "    def forward(self, context, target):\n",
    "        # context: [b_size, 2 * window_size]\n",
    "        # target: [b_size]\n",
    "        neu1 = self.syn0(context.long()).mean(dim=1)  # [b_size, embedding_dim]\n",
    "        loss = self.hs(neu1, target.long())\n",
    "        return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = CBOWDataset(corpus_indexed)\n",
    "data_loader = DataLoader(data_set, batch_size=100, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "net = CBOWHierarchicalSoftmax(vocab_size, embedding_dim, fre_dist_indexed)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001,  weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 361/361 [00:06<00:00, 56.96it/s, loss=8.98]\n",
      "100%|██████████| 361/361 [00:05<00:00, 69.30it/s, loss=8.86]\n",
      "100%|██████████| 361/361 [00:05<00:00, 69.50it/s, loss=8.77]\n",
      "100%|██████████| 361/361 [00:05<00:00, 69.73it/s, loss=8.7]\n",
      "100%|██████████| 361/361 [00:05<00:00, 70.27it/s, loss=8.63]\n",
      "100%|██████████| 361/361 [00:05<00:00, 71.61it/s, loss=8.56]\n",
      "100%|██████████| 361/361 [00:05<00:00, 60.62it/s, loss=8.49]\n",
      "100%|██████████| 361/361 [00:07<00:00, 48.11it/s, loss=8.42]\n",
      "100%|██████████| 361/361 [00:07<00:00, 50.28it/s, loss=8.35]\n",
      "100%|██████████| 361/361 [00:05<00:00, 70.87it/s, loss=8.29]\n",
      "100%|██████████| 361/361 [00:05<00:00, 65.54it/s, loss=8.22]\n",
      "100%|██████████| 361/361 [00:06<00:00, 57.14it/s, loss=8.16]\n",
      "100%|██████████| 361/361 [00:06<00:00, 55.74it/s, loss=8.1]\n",
      "100%|██████████| 361/361 [00:05<00:00, 67.17it/s, loss=8.04]\n",
      "100%|██████████| 361/361 [00:05<00:00, 68.77it/s, loss=7.98]\n",
      "100%|██████████| 361/361 [00:06<00:00, 57.39it/s, loss=7.93]\n",
      "100%|██████████| 361/361 [00:05<00:00, 60.18it/s, loss=7.87]\n",
      "100%|██████████| 361/361 [00:14<00:00, 25.70it/s, loss=7.82]\n",
      "100%|██████████| 361/361 [00:07<00:00, 47.73it/s, loss=7.77]\n",
      "100%|██████████| 361/361 [00:07<00:00, 48.57it/s, loss=7.72]\n"
     ]
    }
   ],
   "source": [
    "log_interval = 100\n",
    "for epoch_i in range(20):\n",
    "    total_loss = 0\n",
    "    net.train()\n",
    "    tk0 = tqdm.tqdm(data_loader, smoothing=0, mininterval=1.0)\n",
    "    for i, (context, center) in enumerate(tk0):\n",
    "\n",
    "        loss = net(context, center)\n",
    "        net.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if(i + 1) % log_interval == 0:\n",
    "            tk0.set_postfix(loss = total_loss/log_interval)\n",
    "            total_loss = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
